{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post请求，提取&调用cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sarah Mosses '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'SARAH MOSSES '.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p data-v-101a27be=\"\">Catholic Answers is pleased to provide this unabridged entry from the original Catholic Encyclopedia, published between 1907 and 1912. It is a valuable resource for subjects related to theology, philosophy, history, culture, and more. Like most works that are more than a century old, though, it may occasionally use anachronistic language or present outdated scientific information. Accordingly, in offering this resource Catholic Answers does not thereby endorse every assertion or phrase in it.</p>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file = open('prose.txt','w')\n",
    "url = 'https://www.catholic.com/encyclopedia/prose-or-sequence'\n",
    "r = requests.get(url)\n",
    "s = BeautifulSoup(r.text,'html.parser')\n",
    "content = s.find('main',id='main-content').find('div',id='content-body').find_all('p')\n",
    "len(content)\n",
    "content\n",
    "\n",
    "\n",
    "# file.write(lyrics)\n",
    "\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('lyrics.txt','r')\n",
    "content = file.read()\n",
    "type(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#引入requests。\n",
    "url = ' https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'\n",
    "#把请求登录的网址赋值给url。\n",
    "headers = {\n",
    "'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'\n",
    "}\n",
    "#加请求头，前面有说过加请求头是为了模拟浏览器正常的访问，避免被反爬虫。\n",
    "data = {\n",
    "'log': 'spiderman',  #写入账户\n",
    "'pwd': 'crawler334566',  #写入密码\n",
    "'wp-submit': '登录',\n",
    "'redirect_to': 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-admin/',\n",
    "'testcookie': '1'\n",
    "}\n",
    "#把有关登录的参数封装成字典，赋值给data。\n",
    "login_in = requests.post(url,headers=headers,data=data)\n",
    "#用requests.post发起请求，放入参数：请求登录的网址、请求头和登录参数，然后赋值给login_in。\n",
    "cookies = login_in.cookies\n",
    "#提取cookies的方法：调用requests对象（login_in）的cookies属性获得登录的cookies，并赋值给变量cookies。\n",
    "\n",
    "url_1 = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-comments-post.php'\n",
    "#我们想要评论的文章网址。\n",
    "data_1 = {\n",
    "'comment': input('请输入你想要发表的评论：'),\n",
    "'submit': '发表评论',\n",
    "'comment_post_ID': '13',\n",
    "'comment_parent': '0'\n",
    "}\n",
    "#把有关评论的参数封装成字典。\n",
    "comment = requests.post(url_1,headers=headers,data=data_1,cookies=cookies)\n",
    "#用requests.post发起发表评论的请求，放入参数：文章网址、headers、评论参数、cookies参数，赋值给comment。\n",
    "#调用cookies的方法就是在post请求中传入cookies=cookies的参数。\n",
    "print(comment.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建session，把cookies转化成字典，并存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入账号：spiderman\n",
      "请输入密码：crawler334566\n"
     ]
    }
   ],
   "source": [
    "import requests,json\n",
    "#引用requests。\n",
    "session = requests.session()\n",
    "#用requests.session()创建session对象，相当于创建了一个特定的会话，帮我们自动保持了cookies。\n",
    "url = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'\n",
    "headers = {\n",
    "'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'\n",
    "}\n",
    "data = {\n",
    "    'log':input('请输入账号：'), #用input函数填写账号和密码，这样代码更优雅，而不是直接把账号密码填上去。\n",
    "    'pwd':input('请输入密码：'),\n",
    "    'wp-submit':'登录',\n",
    "    'redirect_to':'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-admin/',\n",
    "    'testcookie':'1'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.post(url,headers=headers,data=data)\n",
    "#在创建的session下用post发起登录请求，放入参数：请求登录的网址、请求头和登录参数。\n",
    "cookies_dict = requests.utils.dict_from_cookiejar(session.cookies) #dict_from_cookiejar函数将cj中的cookies存成一个dict\n",
    "#把cookies转化成字典。⚠️session.cookies 是Session这个类的实例的方法吗？返回的是什么？⚠️\n",
    "cookies_str = json.dumps(cookies_dict)\n",
    "#调用json模块的dumps函数，把cookies从字典再转成字符串。\n",
    "f = open('cookies.txt','w')\n",
    "f.write(cookies_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "url_1 = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-comments-post.php'\n",
    "#把我们想要评论的文章网址赋值给url_1。\n",
    "data_1 = {\n",
    "'comment': input('请输入你想要发表的评论：'),\n",
    "'submit': '发表评论',\n",
    "'comment_post_ID': '13',\n",
    "'comment_parent': '0'\n",
    "}\n",
    "#把有关评论的参数封装成字典。\n",
    "\n",
    "comment = session.post(url_1,headers=headers,data=data_1)\n",
    "#在创建的session下用post发起评论请求，放入参数：文章网址，请求头和评论参数，并赋值给comment。\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies_txt = open('cookies.txt', 'r')\n",
    "#以reader读取模式，打开名为cookies.txt的文件。\n",
    "cookies_dict = json.loads(cookies_txt.read())\n",
    "#调用json模块的loads函数，把字符串转成字典。\n",
    "cookies = requests.utils.cookiejar_from_dict(cookies_dict)\n",
    "#把转成字典的cookies再转成cookies本来的格式。\n",
    "session.cookies = cookies\n",
    "#获取cookies：就是调用requests对象（session）的cookies属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session cookies (requests, json),课上完整代码解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,json\n",
    "session = requests.session()\n",
    "#创建会话。\n",
    "headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'\n",
    "}\n",
    "#添加请求头，避免被反爬虫。\n",
    "try:\n",
    "#如果能读取到cookies文件，执行以下代码，跳过except的代码，不用登录就能发表评论。\n",
    "    cookies_txt = open('cookies.txt', 'r')\n",
    "    #以reader读取模式，打开名为cookies.txt的文件。\n",
    "    cookies_dict = json.loads(cookies_txt.read())\n",
    "    #调用json模块的loads函数，把字符串转成字典。\n",
    "    cookies = requests.utils.cookiejar_from_dict(cookies_dict)\n",
    "    #把转成字典的cookies再转成cookies本来的格式。\n",
    "    session.cookies = cookies\n",
    "    #获取cookies：就是调用requests对象（session）的cookies属性。\n",
    "\n",
    "except FileNotFoundError:\n",
    "#如果读取不到cookies文件，程序报“FileNotFoundError”（找不到文件）的错，则执行以下代码，重新登录获取cookies，再评论。\n",
    "\n",
    "    url = ' https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'\n",
    "    #登录的网址。\n",
    "    data = {'log': input('请输入你的账号:'),\n",
    "            'pwd': input('请输入你的密码:'),\n",
    "            'wp-submit': '登录',\n",
    "            'redirect_to': 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-admin/',\n",
    "            'testcookie': '1'}\n",
    "    #登录的参数。\n",
    "    session.post(url, headers=headers, data=data)\n",
    "    #在会话下，用post发起登录请求。\n",
    "\n",
    "    cookies_dict = requests.utils.dict_from_cookiejar(session.cookies)\n",
    "    #把cookies转化成字典。\n",
    "    cookies_str = json.dumps(cookies_dict)\n",
    "    #调用json模块的dump函数，把cookies从字典再转成字符串。\n",
    "    f = open('cookies.txt', 'w')\n",
    "    #创建名为cookies.txt的文件，以写入模式写入内容\n",
    "    f.write(cookies_str)\n",
    "    #把已经转成字符串的cookies写入文件\n",
    "    f.close()\n",
    "    #关闭文件\n",
    "\n",
    "url_1 = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-comments-post.php'\n",
    "#文章的网址。\n",
    "data_1 = {\n",
    "'comment': input('请输入你想评论的内容：'),\n",
    "'submit': '发表评论',\n",
    "'comment_post_ID': '13',\n",
    "'comment_parent': '0'\n",
    "}\n",
    "#评论的参数。\n",
    "comment = session.post(url_1,headers=headers,data=data_1)\n",
    "#在创建的session下用post发起评论请求，放入参数：文章网址，请求头和评论参数，并赋值给comment。\n",
    "print(comment.status_code)\n",
    "#打印comment的状态码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解决cookies过期 完整OOP形式的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "session = requests.session()\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'}\n",
    "\n",
    "def cookies_read():\n",
    "    cookies_txt = open('cookies.txt', 'r')\n",
    "    cookies_dict = json.loads(cookies_txt.read())\n",
    "    cookies = requests.utils.cookiejar_from_dict(cookies_dict)\n",
    "    return (cookies)\n",
    "    # 以上4行代码，是cookies读取。\n",
    "\n",
    "def sign_in():\n",
    "    url = ' https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'\n",
    "    data = {'log': input('请输入你的账号'),\n",
    "            'pwd': input('请输入你的密码'),\n",
    "            'wp-submit': '登录',\n",
    "            'redirect_to': 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-admin/',\n",
    "            'testcookie': '1'}\n",
    "    session.post(url, headers=headers, data=data)\n",
    "    cookies_dict = requests.utils.dict_from_cookiejar(session.cookies)\n",
    "    cookies_str = json.dumps(cookies_dict)\n",
    "    f = open('cookies.txt', 'w')\n",
    "    f.write(cookies_str)\n",
    "    f.close()\n",
    "    # 以上5行代码，是cookies存储。\n",
    "\n",
    "\n",
    "def write_message():\n",
    "    url_2 = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-comments-post.php'\n",
    "    data_2 = {\n",
    "        'comment': input('请输入你要发表的评论：'),\n",
    "        'submit': '发表评论',\n",
    "        'comment_post_ID': '13',\n",
    "        'comment_parent': '0'\n",
    "    }\n",
    "    return (session.post(url_2, headers=headers, data=data_2))\n",
    "    #以上9行代码，是发表评论。\n",
    "\n",
    "try:\n",
    "    session.cookies = cookies_read()\n",
    "except FileNotFoundError:\n",
    "    sign_in()\n",
    "    session.cookies = cookies_read()\n",
    "\n",
    "num = write_message()\n",
    "if num.status_code == 200:\n",
    "    print('成功啦！')\n",
    "else:\n",
    "    sign_in()\n",
    "    session.cookies = cookies_read()\n",
    "    num = write_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取电影名 评分 链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先爬取最小共同父级标签<li>，然后针对每一个父级标签，提取里面的序号/电影名/评分/推荐语/链接。\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for i in range(10):\n",
    "    url = 'https://movie.douban.com/top250?start='+str(i*25)+'&filter='\n",
    "    r = requests.get(url)\n",
    "    s = BeautifulSoup(r.text,'html.parser')\n",
    "    info = s.find_all('div','item')\n",
    "    for i in info:\n",
    "        with open('Top250 douban.txt','a') as t:\n",
    "            num = i.find('div','pic').em.text\n",
    "            title = i.find('div','info').a.text.replace('\\n','')\n",
    "            rate = i.find('span','rating_num').text\n",
    "            t.write(num+' '+title+'\\n')\n",
    "            t.write(' '*len(num)+' '+'rating: '+rate)\n",
    "            \n",
    "            if i.find('p','quote')!=None:\n",
    "                quote = i.find('p','quote').text.replace('\\n','')\n",
    "                t.write('\\t\\\"'+quote+'\\\"\\n')\n",
    "            else:\n",
    "                t.write('\\t-\\n')\n",
    "            \n",
    "            link = i.find('div','info').a['href']\n",
    "            t.write(' '*len(num)+' '+link+'\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://c.y.qq.com/soso/fcgi-bin/client_search_cp'\n",
    "\n",
    "params = {\n",
    "    'ct': '24',\n",
    "    'qqmusic_ver': '1298',\n",
    "    'new_json': '1',\n",
    "    'remoteplace': 'sizer.yqq.song_next',\n",
    "    'searchid': '64405487069162918',\n",
    "    't': '0',\n",
    "    'aggr': '1',\n",
    "    'cr': '1',\n",
    "    'catZhida': '1',\n",
    "    'lossless': '0',\n",
    "    'flag_qc': '0',\n",
    "    'p': '1',\n",
    "    'n': '20',\n",
    "    'w': '滨崎步',\n",
    "    'g_tk': '5381',\n",
    "    'loginUin': '0',\n",
    "    'hostUin': '0',\n",
    "    'format': 'json',\n",
    "    'inCharset': 'utf8',\n",
    "    'outCharset': 'utf-8',\n",
    "    'notice': '0',\n",
    "    'platform': 'yqq.json',\n",
    "    'needNewCode': '0'\n",
    "}\n",
    "\n",
    "res_music = requests.get(url, params=params)\n",
    "print(res_music)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in info:\n",
    "#     if info.index(i)>=3:\n",
    "#         break\n",
    "#     else:\n",
    "    title = i.a.text\n",
    "    link = 'https://www.ygdy8.com'+i.a['href']\n",
    "    print('第{}条结果：'.format(info.index(i)+1))\n",
    "    print(title)\n",
    "    print('影片页面直达：'+link) \n",
    "    res = requests.get(link)\n",
    "    print(res.status_code)\n",
    "    res.encoding = 'gbk'\n",
    "    s = BeautifulSoup(res.text,'html.parser')\n",
    "    download_link = s.find('div',id='Zoom').table.a.text\n",
    "    print(download_link)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "big_list = []\n",
    "r = requests.get('http://www.xiachufang.com/explore/')\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "info = soup.find_all('p','name')\n",
    "ings = soup.find_all('p','ing ellipsis')\n",
    "\n",
    "whole_list = []\n",
    "for i in range(len(info)):\n",
    "    dish_list = [info[i].text.lstrip().rstrip(),\n",
    "                  ings[i].text.lstrip().rstrip(),url_head+info[i].a['href']]\n",
    "    whole_list.append(dish_list)\n",
    "print(len(whole_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "big_list = []\n",
    "r = requests.get('http://www.xiachufang.com/explore/')\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "info = soup.find_all('p','name')\n",
    "ings = soup.find_all('p','ing ellipsis')\n",
    "\n",
    "url_head = 'http://www.xiachufang.com'\n",
    "dish_list = [dish.text.lstrip().rstrip() for dish in info]\n",
    "url_list = [url_head+tag.a['href'] for tag in info]\n",
    "ing_list = [ing.text.lstrip().rstrip() for ing in ings]\n",
    "for i in range(len(dish_list)):\n",
    "    dish,url,ing = dish_list[i],url_list[i],ing_list[i]\n",
    "    print(dish,url,ing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, random, bs4\n",
    "url = 'https://movie.douban.com/top250?start=0&filter='\n",
    "r = requests.get(url)\n",
    "r.status_code\n",
    "soup = bs4.BeautifulSoup(r.text,'html.parser')\n",
    "for tag in soup.find_all('span','title'):\n",
    "    title = tag.text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一键下载电影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一键下载电影\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import quote\n",
    "\n",
    "client_search = True\n",
    "while client_search:\n",
    "    search = input('请输入电影中文名称：')\n",
    "    search_title = quote(search.encode('gbk'))\n",
    "    url = 'http://s.ygdy8.com/plus/so.php?typeid=1&keyword='+search_title\n",
    "    print('搜索结果页面直达：'+url)\n",
    "    r = requests.get(url)\n",
    "    r.encoding = 'gbk'\n",
    "    s = BeautifulSoup(r.text,'html.parser')\n",
    "    info = s.find('div','co_content8').find('ul').find_all('b') #all search results\n",
    "    if len(info)!=0:\n",
    "        print('（共{}条搜索结果）'.format(len(info)))\n",
    "        if len(info)>=10:\n",
    "            print('注：搜索结果较多，请确认片名的完整性。')\n",
    "            user_choice = input('重新搜索请输入0，按其他任意键继续：')\n",
    "            if user_choice=='0':\n",
    "                continue\n",
    "            else:\n",
    "                client_search = False\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    else:\n",
    "        print('no movie')\n",
    "        continue\n",
    "\n",
    "for i in info:\n",
    "    if info.index(i)>=3:\n",
    "        break\n",
    "    else:\n",
    "        title = i.a.text\n",
    "        link = 'https://www.ygdy8.com'+i.a['href']\n",
    "        print('第{}条结果：'.format(info.index(i)+1))\n",
    "        print(title)\n",
    "        print('影片页面直达：'+link) \n",
    "        r = requests.get(link)\n",
    "        r.encoding = 'gbk'\n",
    "        s = BeautifulSoup(r.text,'html.parser')\n",
    "        download_link = s.find('div',id='Zoom').table.a.text\n",
    "        print(download_link)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.meijutt.tv/')\n",
    "time.sleep(3)\n",
    "mName = driver.find_element_by_name('searchword')\n",
    "mName.clear()\n",
    "mName.send_keys('名姝')\n",
    "button = driver.find_element_by_id('keyword_bnt')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xc3\\xfb\\xe6\\xad'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'名姝'.encode('GBK')\n",
    "#b'%C3%FB%E6%AD'.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<style>body{text-align:center}#msg{background-color:white;border:1px solid #1B76B7;margin:0 auto;width:400px;text-align:left}.msgtitle{padding:3px 3px;color:white;font-weight:700;line-height:21px;height:25px;font-size:12px;border-bottom:1px solid #1B76B7; text-indent:3px; background-color:#1B76B7}#msgbody{font-size:12px;padding:40px 8px 50px;line-height:25px}#msgbottom{text-align:center;height:20px;line-height:20px;font-size:12px;background-color:#1b76b7;color:#FFFFFF}</style><div id='msg'><div class='msgtitle'>【警告】非法提交：</div><div id='msgbody'>你提交的数据有非法字符，这是为了防止攻击设置的，建议搜索中文名,操作时间:2020/1/5 22:54:16</div><div id='msgbottom'>Powered By 美剧天堂</div></div>\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.meijutt.tv/search/index.asp'\n",
    "data = {'searchword':'%C3%FB%E6%AD'}\n",
    "r = requests.post(url,data=data)\n",
    "r.encoding = 'GBK'\n",
    "r.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, html\n",
    "url = 'https://c.y.qq.com/soso/fcgi-bin/client_search_cp?ct=24&qqmusic_ver=1298&remoteplace=txt.yqq.lyric&searchid=106425454264060565&aggr=0&catZhida=1&lossless=0&sem=1&t=7&p=2&n=5&w=%E5%91%A8%E6%9D%B0%E4%BC%A6&g_tk=5381&loginUin=0&hostUin=0&format=json&inCharset=utf8&outCharset=utf-8&notice=0&platform=yqq.json&needNewCode=0'\n",
    "\n",
    "# headers = {\n",
    "#     'origin':'https://y.qq.com',\n",
    "#     # 请求来源，本案例中其实是不需要加这个参数的，只是为了演示\n",
    "#     'referer':'https://y.qq.com/n/yqq/song/004Z8Ihr0JIu5s.html',\n",
    "#     # 请求来源，携带的信息比“origin”更丰富，本案例中其实是不需要加这个参数的，只是为了演示\n",
    "#     'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',\n",
    "#     # 标记了请求从什么设备，什么浏览器上发出\n",
    "#     }\n",
    "# # 伪装请求头\n",
    "\n",
    "# params = {\n",
    "# 'ct':'24',\n",
    "# 'qqmusic_ver': '1298',\n",
    "# 'new_json':'1',\n",
    "# 'remoteplace':'sizer.yqq.song_next',\n",
    "# 'searchid':'64405487069162918',\n",
    "# 't':'0',\n",
    "# 'aggr':'1',\n",
    "# 'cr':'1',\n",
    "# 'catZhida':'1',\n",
    "# 'lossless':'0',\n",
    "# 'flag_qc':'0',\n",
    "# 'p':1,\n",
    "# 'n':'20',\n",
    "# 'w':'周杰伦',\n",
    "# 'g_tk':'5381',\n",
    "# 'loginUin':'0',\n",
    "# 'hostUin':'0',\n",
    "# 'format':'json',\n",
    "# 'inCharset':'utf8',\n",
    "# 'outCharset':'utf-8',\n",
    "# 'notice':'0',\n",
    "# 'platform':'yqq.json',\n",
    "# 'needNewCode':'0'    \n",
    "# }\n",
    "# 将参数封装为字典\n",
    "res_music = requests.get(url,headers=headers,params=params)\n",
    "# 发起请求，填入请求头和参数\n",
    "json_music = res_music.json()\n",
    "lyric_list = json_music['data']['lyric']['list']\n",
    "for lyric in lyric_list:\n",
    "    l = html.unescape(lyric['content'].replace('\\\\n','\\n'))\n",
    "    print(l)\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrap data and write into excel & csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,bs4,openpyxl\n",
    "# from time import datetime\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.active\n",
    "sheet.title = 'top250豆瓣'\n",
    "head = ['rank','movie title','rating','recommend','page link']\n",
    "sheet.append(head)\n",
    "\n",
    "for x in range(10):\n",
    "    url = 'https://movie.douban.com/top250?start=' + str(x*25) + '&filter='\n",
    "    header = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}\n",
    "    res = requests.get(url,headers=header)\n",
    "    bs = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    bs = bs.find('ol', class_=\"grid_view\")\n",
    "    for titles in bs.find_all('li'):\n",
    "        rank = titles.find('em',class_=\"\").text\n",
    "        movie_title = titles.find('span', class_=\"title\").text\n",
    "        rating = titles.find('span',class_=\"rating_num\").text\n",
    "        movie_link = titles.find('a')['href']\n",
    "        if titles.find('span',class_=\"inq\") != None:\n",
    "            rec = titles.find('span',class_=\"inq\").text\n",
    "        else:\n",
    "            rec = '-'\n",
    "        sheet.append([rank,movie_title,rating,rec,movie_link])\n",
    "        \n",
    "wb.save('top250 by openpyxl.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,bs4,csv\n",
    "# from time import datetime\n",
    "\n",
    "csvFile = open('top250 by csv.csv','a')\n",
    "writer = csv.writer(csvFile)#,encoding='utf-8-sig')\n",
    "head = ['rank','movie title','rating','recommend','page link']\n",
    "writer.writerow(head)\n",
    "\n",
    "for x in range(10):\n",
    "    url = 'https://movie.douban.com/top250?start=' + str(x*25) + '&filter='\n",
    "    header = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}\n",
    "    res = requests.get(url,headers=header)\n",
    "    bs = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    bs = bs.find('ol', class_=\"grid_view\")\n",
    "    for titles in bs.find_all('li'):\n",
    "        rank = titles.find('em',class_=\"\").text\n",
    "        movie_title = titles.find('span', class_=\"title\").text\n",
    "        rating = titles.find('span',class_=\"rating_num\").text\n",
    "        movie_link = titles.find('a')['href']\n",
    "        if titles.find('span',class_=\"inq\") != None:\n",
    "            rec = titles.find('span',class_=\"inq\").text\n",
    "        else:\n",
    "            rec = '-'\n",
    "        writer.writerow([rank,movie_title,rating,rec,movie_link])\n",
    "        \n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 饿了么 爬虫参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "session = requests.session()\n",
    "\n",
    "headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'\n",
    "}\n",
    "url_1 = 'https://h5.ele.me/restapi/eus/login/mobile_send_code'\n",
    "tel = input('请输入手机号码：')\n",
    "data_1 = {'captcha_hash':'',\n",
    "        'captcha_value':'',\n",
    "        'mobile':tel,\n",
    "        'scf':''}\n",
    "\n",
    "token = session.post(url_1, headers=headers, data=data_1).json()['validate_token']\n",
    "\n",
    "url_2 = 'https://h5.ele.me/restapi/eus/login/login_by_mobile'\n",
    "code = input('请输入手机验证码：')\n",
    "data_2 = {'mobile':tel,\n",
    "        'scf':'ms',\n",
    "        'validate_code':code,\n",
    "        'validate_token':token}\n",
    "\n",
    "session.post(url_2,headers=headers,data=data_2)\n",
    "\n",
    "\n",
    "address_url = 'https://www.ele.me/restapi/v2/pois?'\n",
    "place = input('请输入你的收货地址：')\n",
    "params = {'extras[]':'count','geohash':'ws105rz9smwm','keyword':place,'limit':'20','type':'nearby'}\n",
    "# 这里使用了深圳的geohash\n",
    "\n",
    "address_res = requests.get(address_url,params=params)\n",
    "address_json = address_res.json()\n",
    "\n",
    "print('以下，是与'+place+'相关的位置信息：\\n')\n",
    "n=0\n",
    "for address in address_json:\n",
    "    print(str(n)+'. '+address['name']+'：'+address['short_address']+'\\n')\n",
    "    n = n+1\n",
    "address_num = int(input('请输入您选择位置的序号：'))\n",
    "final_address = address_json[address_num]\n",
    "\n",
    "restaurants_url = 'https://www.ele.me/restapi/shopping/restaurants?'\n",
    "# 使用带有餐馆列表的那个XHR地址。\n",
    "params = {'extras[]':'activities',\n",
    "'geohash':final_address['geohash'],\n",
    "'latitude':final_address['latitude'],\n",
    "'limit':'24',\n",
    "'longitude':final_address['longitude'],\n",
    "'offset':'0',\n",
    "'terminal':'web'\n",
    "}\n",
    "# 将参数封装，其中geohash和经纬度，来自前面获取到的数据。\n",
    "restaurants_res = session.get(restaurants_url,params=params)\n",
    "# 发起请求，将响应的结果，赋值给restaurants_res\n",
    "restaurants = restaurants_res.json()\n",
    "# 把response对象，转为json。\n",
    "for restaurant in restaurants:\n",
    "# restsurants最外层是一个列表，它可被遍历。restaurant则是字典，里面包含了单个餐厅的所有信息。\n",
    "    print(restaurant['name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
